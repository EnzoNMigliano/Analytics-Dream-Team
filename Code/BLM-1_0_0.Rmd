---
title: "Black Lives Matter - Analytics Dream Team"
author: "Adriana Ortiz, Enzo Novi Migliano, and Juan Zambrano"
output: html_notebook
---

# Abstract


The lives of African Americans have historically been disrespected. The treatment disparity of different ethnicities in the American society has a long and diverse scope. However, Under the following research question and problem statement: Do African Americans are significantly treated different in society than to other ethnicities in the United States? The presented study focused on the facets of the education, health, and law enforcement societal interaction. Through an archival analysis of governmental and public available data different statistical tests such as time series, linear and non-linear predictive models, and descriptive statistics will address the following hypothesis: Are there differences in the incidents of scholls of different neighboor in New York (i,e,. majorly Caucasians neighboorhoods, and majorly African American neighboorhoods), is the number of African American being shot gonna go up or down in the follwing years. The current study provided great insight on the current situation of the Ethnical disparity of the American, as well possible solutions for the problem **<We Need to make a better COnclusion>**.


# Black Lives Matter

Black Lives Matter (BLM) is a movement that started as a hashtag for Twitter, and it has evolved to a political and social movement, aiming to visualize and intervene in violence inflicted on Black communities, under the BLM movements many protests have been promoted around the globe advocating against police violence towards black people. The main focus of the movement is to end with police brutality.

To understand the facts behind the claim of BLM, we are going to examine data from US authorities and how they treat people to assess if there is a difference in the way they treat Black Americans. The supporting data used towards this project comes from the US Census Bureau, New York Police Department, open data sources from some main metro US areas (Chicago, Dallas & LA), demographics including education, housing, and politics, and lastly data from the police authority. By using different statistical analysis techniques we aim to answer some questions such as how the police treatment towards black people has changed over the years, are African Americans being arrested more often than white, whats the group age seeing more disparity in police treatment among others.




# Method

## Data

The data utilized in the analysis was collected from different reliable sources (e.g., government, news papers, Kaggle). The data was collected in throughout the months of September and October of 2020, and the files were uploaded into an online repository in the web server "GitHub" (link to the online repository:
https://github.com/EnzoNMigliano/Analytics-Dream-Team).


## Instruments

### SOftware

In order to perform the analysis of the data we utilized the software R(Version 4.0.2) [1], emulated in the Rstudio(Version 1.3.1073) [2]. In order to perform the version control of the documents throughout the study, the team utilized the Software Git [3] in connection with the server of GitHub [4].

### Packages

In order to perform the analysis of the data we utilized the following packages: Tidyverse [5], Corrplot [6], CaTools [7], caret [8], and forecast [9].

#### Installing Necessary Packages

The code below is with hash tags to prevent people from installing the packages twice. If your computer does not have one of the packages below, please take way the hash tag and install the package at your convenience.

```{r}

# install.packages("tidyverse")
# install.packages("corrplot")
# install.packages("caTools")
# install.packeges("caret")
# install.packages("forecast")

```


#### Loading Necessary Libraries

The code below will load the installed packages into your Rstudio.

```{r}
library(tidyverse)
library(corrplot)
library(caTools)
library(caret)
library(forecast)
```

## Procedures

### Loading Data sets for the analysis

The code below will load all the data sets the team stored in the GitHub server.

```{r}
# Analytics-Dream-Team/Data

Covid_Deaths_Race <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/Provisional_COVID-19_Death_Counts_by_County_and_Race_CDC_OCT1.csv")

All_Illness_Deaths_Rate <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/Cumulative_Provisional_Counts_of_Deaths_by_Sex__Race__and_Age_CDC_AUG10.csv")

'2010&2011_Vadir_NY' <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/2010-2011_VADIR_INCIDENTS-CITY_OF_NEW_YORK.csv")

'2012&2013_Vadir_NY' <- 
  read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/2012-2013_VADIR_INCIDENTS-CITY_OF%20_NEW%20YORK.csv")

'2014&2015_Vadir_NY' <- 
  read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/2014-2015_VADIR_INCIDENTS-CITY_OF_NEW_YORK.csv")

# Analytics-Dream-Team/Data/BLM Diverse Data/police_info/

Police_Budget <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/police_info/budgets.csv")

Equipment_Police <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/police_info/dod_equipment_purchases.csv")

Contract_Police <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/police_info/police_contracts.csv")


Police_Policies <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/police_info/police_policies.csv")

# Analytics-Dream-Team/Data/BLM Diverse Data/demographics/

Poverty_Census <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/demographics/poverty_census_bureau.csv")

Politics_538 <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/demographics/politics_538.csv")

Housing <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/demographics/housing.csv")

Eduction_Census <- read.csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/demographics/education_census_bureau.csv")

# Analytics-Dream-Team/Data/BLM Diverse Data/crime_data/

Juvenile_Arrests <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/crime_data/juvenile_arrests.csv")


# Analytics-Dream-Team/Data/BLM Diverse Data/NYC_stop_frisk/

NY_StopFrisk_2015 <- 
  read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/NYC_stop_frisk/sqf-2015.csv")

NY_StopFrisk_2016 <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/NYC_stop_frisk/sqf-2016.csv")

NY_StopFrisk_2017 <- read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/BLM%20Diverse%20Data/NYC_stop_frisk/sqf-2017.csv")

  
Washington_post_Shootings <-
  read_csv("https://raw.githubusercontent.com/EnzoNMigliano/Analytics-Dream-Team/Backup/Data/shootings_wash_post.csv")

```



#### Brief description of the data sets

##### **VADIR_NY**


This is a conjunct of three data sets on reports of different schools according to its neighborhood in New York City. The reports range from fights to murder. There are three data sets with data collected by the local government of New York City from 2010 to 2015.


##### **All_Illnesses_Deaths_rate**


This data set is maintained by the Center of Disease Control and Prevention. It contains the most recent updated in all the most prevalent cause of deaths in the American society. THe data set is detailed and it gives the information regarding the different ethnicities in the American society.



##### **COntract_Police**


This data set maintained by the non governmental organization "Check the Police" lists the contract provisions with the police unions in the United States.


##### **Covid_Deaths_Race**


This data set maintained by the Center for Disease COntrol and Prevention lists the deaths by ethnicity in the united States up to August of 2020. 



##### **Eduction_Census**


This Data maintained by the United States Census Bureau lists several indicators for the education in the United States. The data represents the most up to date available data from the Census Bureau. 



##### **Equipment_police**


This data set is maintained by the Department of Defense 
and has information on most equipment purchases by State, City, and County. 

##### **Housing**

This data set is data set hosted by the Census Bureau and contains an approximate calculation of the housing occupancy, housing units, vacant units per state.

##### **Juvenile_Arrests**

This data set was created by Bureau of Justice, it reflects Juvenile Arrest rates sorted from 1980 to 2018.



##### **NY_StopFrisk_2015**


This data set was created by New York police department. It expoxes all stop and frisk activity in the City of New York in 2015.


##### **NY_StopFrisk_2016**

This data set was created by New York police department. It expoxes all stop and frisk activity in the City of New York in 2016.



##### **NY_StopFrisk_2017**

This data set was created by New York police department. It expoxes all stop and frisk activity in the City of New York in 2017.


##### **NY_StopFrisk_2018**


This data set was created by New York police department. It expoxes all stop and frisk activity in the City of New York in 2015.

##### **NY_StopFrisk_2016**

This data set was created by New York police department. It expoxes all stop and frisk activity in the City of New York in 2016.

##### **NY_StopFrisk_2017**

This data set was created by New York police department. It expoxes all stop and frisk activity in the City of New York in 2017.


##### **Police_Budget**


This dataset contais information about the finances for 150 large US ities across 120 categories including population, revenue per city and taxes.

####**Politics**
DataSet containing the political party inclination and segregation for metro areas.

####**Poverty Census**
The Dataset is about poverty levels by metro area.

####**Washinton Post Shootings**
This dataset contains names and data for citizens fatally shot by police.






### Tidying the Data Sets and exploring the data sets 

Data sets contain incredible value for the most diverse analysis. However, in order to complete extract such intrinsic value all data sets must be tidy, in order words, the data sets must organized in a logical manner for the analysis. In the next section all the tidying performed in the study is registered. In order to tidy the data, the team also explored it and got familiarized with data sets by using funciton such as View() and summary(). 


#### Tidying

This section contains the names of the data sets and it status regarding its organization.

##### **VADIR_NY**


In order to check if the data is tidy lets visualize the data set.

*2010-2011*

```{r}
View(`2010&2011_Vadir_NY`)
`2010&2011_Vadir_NY`
```

```{r}
summary(`2010&2011_Vadir_NY`)
```

*2012-2013*

```{r}
View(`2012&2013_Vadir_NY`)
`2012&2013_Vadir_NY`
```

```{r}
summary(`2012&2013_Vadir_NY`)

```

*2014-2015*

```{r}
View(`2014&2015_Vadir_NY`)
`2014&2015_Vadir_NY`
```

```{r}
summary(`2014&2015_Vadir_NY`)
```

All the New York Schools data sets are tidy, no further action required.


##### **All_Illnesses_Deaths_rate**


In order to check if the data is tidy lets visualize the data set.


```{r}
View(All_Illness_Deaths_Rate)
All_Illness_Deaths_Rate
```

```{r}
summary(All_Illness_Deaths_Rate)
```

This data set is tidy, no further action required.



##### **Contract_Police**

There are, however some observaitons that are NA.

```{r}
summary(ChicagoTidy2)
```
The code below will rename the observations that were NA:

```{r}
  
ChicagoTidyFinal <- ChicagoTidy2 %>%
  replace_na(list( 'X Coordinate' = "Missing",
                    'Y Coordinate' = "Missing",
                    Latitude ="Missing", 
                   Longitude = "Missing",
                   Location = "Missing"))

```

The data is now tidy, no further action. See below:

```{r}
ChicagoTidyFinal
```




##### **COntract_Police**


In order to check if the data is tidy lets visualize the data set.

```{r}
View(Contract_Police)
Contract_Police
```
```{r}
summary(Contract_Police)
```

This data set is tidy, no further action required.


##### **Covid_Deaths_Race**


In order to check if the data is tidy lets visualize the data set.


```{r}
view(Covid_Deaths_Race)
Covid_Deaths_Race
```

```{r}
summary(Covid_Deaths_Race)
```


There are some observations with NA. However, We cannot change such observations. 


This data set is tidy, no further action required.




##### **Eduction_Census**


In order to check if the data is tidy lets visualize the data set.

```{r}
View(Eduction_Census)
Eduction_Census
```


The data set Education Census is not tidy yet. There is one column that cointaing two different types of observations. The variable "Geographic.Area.Name" has three observations in it there is the name of the city, the State, and the classification of the city. Therefore, we need to separate those observations into completly new variables by assign it to new columns.

```{r}

Education_Census_Tidy <- Eduction_Census %>%
  separate(Geographic.Area.Name,
           into = c("CityName",
                    "StateName"),
           sep = ", ") %>%
  separate(StateName,
           into = c("State",
                    "Area"),
           sep = " ")

```


*The tidy data set*

The tidy data set will be called "Education_Census_Tidy". In such data set, we transformed through the code above the variable (of the original data set) "Geographic.Area.Name" into the variables "CityName", "State", and "Area"(in the tidy data set). See the new data set below:


```{r}
Education_Census_Tidy
```


##### **Housing**

In order to check if the data is tidy lets visualize the data set.

##### **Equitment_Police**

In order to check if the data is tidy lets visualize the data set.

```{r}
view(Equipment_Police)
Equipment_Police
```

This data set is tidy, no further action required.


##### **Housing**

In order to check if the data is tidy lets visualize the data set.


```{r}
view(Housing)
Housing
```

This data set is tidy, no further action required.


##### **Juvenile_Arrests**

In order to check if the data is tidy lets visualize the data set.

```{r}
view(Juvenile_Arrests)
Juvenile_Arrests
```

This data set is tidy, no further action required.



#### **NY_StopFrisk_2015**

In order to check if the data is tidy lets visualize the data sets.

```{r}
view(NY_StopFrisk_2015)


NY_StopFrisk_2015
```

We are subseting the data with relevant values

```{r}
NY_StopFrisk_2015Tidy <- NY_StopFrisk_2015 %>% subset(select = c(year, datestop, crimsusp, explnstp, 
                                                                  offunif, frisked, searched, contrabn,pf_hcuff, forceuse, sex, race, age, ht_feet, weight, build, city)) %>% rename(Year = year, `Date of Frisk` = datestop,`Crime Suspected` = crimsusp, `Officer Explained Reason` = explnstp, `Officer Wearing Uniform` = offunif, Frisked = frisked, `Body Searched` = searched, Contraband = contrabn, `Hanfcuff Used` = pf_hcuff, `Force Used` = forceuse, Sex = sex, Race = race, Age = age, Height = ht_feet, Weight = weight, `Body Type` = build, City = city)

NY_StopFrisk_2015Tidy

```

This data set is tidy, no further action required.


#### **NY_StopFrisk_2016**

In order to check if the data is tidy lets visualize the data sets.

```{r}
view(NY_StopFrisk_2016)

```

We are subseting the data with relevant values

```{r}
NY_StopFrisk_2016Tidy <- NY_StopFrisk_2016 %>% subset(select = c(year, datestop, crimsusp, explnstp, 
                                                                  offunif, frisked, searched, contrabn,pf_hcuff, forceuse, sex, race, age, ht_feet, weight, build, city)) %>% rename(Year = year, `Date of Frisk` = datestop,`Crime Suspected` = crimsusp, `Officer Explained Reason` = explnstp, `Officer Wearing Uniform` = offunif, Frisked = frisked, `Body Searched` = searched, Contraband = contrabn, `Hanfcuff Used` = pf_hcuff, `Force Used` = forceuse, Sex = sex, Race = race, Age = age, Height = ht_feet, Weight = weight, `Body Type` = build, City = city)
                 

NY_StopFrisk_2016Tidy$Age <- as.double(NY_StopFrisk_2016Tidy$Age)                    

NY_StopFrisk_2016Tidy

```

This data set is tidy, no further action required.


#### **NY_StopFrisk_2017**


In order to check if the data is tidy lets visualize the data sets.

```{r}
view(NY_StopFrisk_2017)
NY_StopFrisk_2017
```


We are subseting the data with relevant values

```{r}
NY_StopFrisk_2017Tidy <- NY_StopFrisk_2017 %>% subset(select = c(YEAR2, STOP_FRISK_DATE,SUSPECTED_CRIME_DESCRIPTION, OFFICER_EXPLAINED_STOP_FLAG, 
                                                                  OFFICER_IN_UNIFORM_FLAG, FRISKED_FLAG, SEARCHED_FLAG, OTHER_CONTRABAND_FLAG,
                                                                 PHYSICAL_FORCE_HANDCUFF_SUSPECT_FLAG, PHYSICAL_FORCE_RESTRAINT_USED_FLAG, SUSPECT_SEX, SUSPECT_RACE_DESCRIPTION, 
                                                                 SUSPECT_REPORTED_AGE, SUSPECT_HEIGHT, SUSPECT_WEIGHT, SUSPECT_BODY_BUILD_TYPE, STOP_LOCATION_BORO_NAME)) %>% rename(Year = YEAR2,`Date of Frisk` = STOP_FRISK_DATE,`Crime Suspected` = SUSPECTED_CRIME_DESCRIPTION, `Officer Explained Reason` = OFFICER_EXPLAINED_STOP_FLAG, `Officer Wearing Uniform` = OFFICER_IN_UNIFORM_FLAG, Frisked = FRISKED_FLAG, `Body Searched` = SEARCHED_FLAG, Contraband = OTHER_CONTRABAND_FLAG, `Hanfcuff Used` = PHYSICAL_FORCE_HANDCUFF_SUSPECT_FLAG, `Force Used` = PHYSICAL_FORCE_RESTRAINT_USED_FLAG, Sex = SUSPECT_SEX, Race = SUSPECT_RACE_DESCRIPTION, Age = SUSPECT_REPORTED_AGE, Height = SUSPECT_HEIGHT, Weight = SUSPECT_WEIGHT, `Body Type` = SUSPECT_BODY_BUILD_TYPE, City = STOP_LOCATION_BORO_NAME)
                                                                  
                                                         

NY_StopFrisk_2017Tidy
```


This data set is tidy, no further action required.



##### **Juvenile_Arrests**

THis data set is tidy.

```{r}
view(Juvenile_Arrests)
```



####**Police Budget**

Data contains missing observations when the city does not percive income per the variable item. , but is tidy

```{r}
Police_Budget
```


####**Politics**

Data is tidy.

```{r}
Politics_538
```

####**Poverty Census**

The data set is tidy.

```{r}
Poverty_Census
```


####**Washinton Post Shootings**

The data set is tidy.
 
```{r}
Washington_post_Shootings

```






### Merging Relational Data

The data collected for the study has several distinct data sets. Some data sets, however, have an intrinsic relation with each other, in other words they have a relation in the observations that they recorded. For example, the data sets about the stop and frisk in New York City are almost the same, the difference is the time in which the observations were recorded (i.e., the year in which it was recorded). Therefore, such data sets can be merged into one for a more meaningful analysis with more observations. The current study merged the following data sets together:



#### **VADIR_NY**

First we will observe the three different data sets:

```{r}
`2010&2011_Vadir_NY`
```

In specific the county variable of each one of the data sets

```{r}
`2010&2011_Vadir_NY` %>% group_by(County) %>% summarise(count = n())
```
```{r}
`2012&2013_Vadir_NY` %>% group_by(County) %>% summarise(count = n())
```

```{r}
`2014&2015_Vadir_NY` %>% group_by(County) %>% summarise(count = n())
```
 There are some variables to rename in the first two data sets, they are the same observations but had different names at different times. See the code below:
 
 
These are the variables of 2010 that were not matched in the data of 2014.

```{r}
`2010&2011_Vadir_NY`[`2010&2011_Vadir_NY`$County == "BRONX"
  , c("County")] <- "Bronx"
```

```{r}
`2010&2011_Vadir_NY`[`2010&2011_Vadir_NY`$County == "NYC CENTRAL OFFICE"
  , c("County")] <- "New York"
```

```{r}
`2010&2011_Vadir_NY`[`2010&2011_Vadir_NY`$County == "BROOKLYN"
  , c("County")] <- "Kings"
```

```{r}
`2010&2011_Vadir_NY`[`2010&2011_Vadir_NY`$County == "MANHATTAN"
  , c("County")] <- "New York"
```

```{r}
`2010&2011_Vadir_NY`[`2010&2011_Vadir_NY`$County == "QUEENS"
  , c("County")] <- "Queens"
```

```{r}
`2010&2011_Vadir_NY`[`2010&2011_Vadir_NY`$County == "RICHMOND"
  , c("County")] <- "Richmond"
```

These are the variables of 2012 that were not matched in the data of 2014.

```{r}
`2012&2013_Vadir_NY`[`2012&2013_Vadir_NY`$County == "Nyc Central Office"
  , c("County")] <- "New York"
```

This data set the same variables being observed over a few years. Therefore, it can be merged into one data set, as bellow:

```{r}
Vadir_NY <- full_join(`2010&2011_Vadir_NY`, `2012&2013_Vadir_NY`)

Vadir_NY_Join <- full_join(Vadir_NY, `2014&2015_Vadir_NY`)
```

The new data set merged all the observations from 2010 to 2015 since it had the same column names (variable names). The new data set is called "Vadir_NY_Join" as below:


```{r}
Vadir_NY_Join
```


#### **NY_StopFrisk_Merged**




We are merging the data sets as they are equal both in rows and columns. First 2015 and 2016



```{r}
NY_StopFrisk1 <- full_join(NY_StopFrisk_2015Tidy, NY_StopFrisk_2016Tidy)
```

```{r}
NY_StopFrisk1
```


Then, the merged data set with 2017


In order to do so, there some issues between the data sets regarding its compatibility. THerefore, the colunm were fixed with the code below:

```{r}
NY_StopFrisk1$`Day Of Frisk` <-  str_sub(NY_StopFrisk1$`Date of Frisk`, 1, 2)
NY_StopFrisk1$`Date of Friskm` <-  NY_StopFrisk1$`Date of Frisk`
str_sub(NY_StopFrisk1$`Date of Friskm`, 2, 2) <- "0"
str_sub(NY_StopFrisk1$`Date of Friskm`, 1, 1) <- "-"
NY_StopFrisk1$`Month Of Frisk` <-  str_sub(NY_StopFrisk1$`Date of Friskm`, 1, 3)
NY_StopFrisk1$`Date of Frisky` <-  NY_StopFrisk1$`Date of Frisk`
str_sub(NY_StopFrisk1$`Date of Frisky`, 3, 3) <- "-"
NY_StopFrisk1$`Year Of Frisk` <-  str_sub(NY_StopFrisk1$`Date of Frisky`, 3, 7)
```


THe data set from 2017 has the variable "Date of Frisk " separated by - and in the order of year, month, and day. Therefore, we need to fix the variable "Date of Frisk" in the "Ny_StopFrisk1" so it able to merge both data sets.

In the code above we divided the variable from "Ny_StopFrisk1" in to day, month and year. Later we substituted the one of the number of the string in another variable so it would be similar to the format in the 2017 data set that contains"-".



```{r}
NY_StopFrisk1
```


Now that all the variables are in the right pieces of the original variable, the function unite was utilized to unite the columns with the values from the original "Date of Frisk", but now in the same formar as is in 2017. See the code below:

```{r}
NY_StopFrisk1_1 <-
  NY_StopFrisk1 %>%
  unite(`Date of Frisk`,`Year Of Frisk`, `Month Of Frisk`,`Day Of Frisk`, `Year Of Frisk`) %>% 
  select(Year, `Date of Frisk`, `Crime Suspected`, `Officer Explained Reason`, `Officer Wearing Uniform`, Frisked, `Body Searched`, Contraband, `Hanfcuff Used`, `Force Used`, Sex, Race, Age, Height, Weight, `Body Type`, City)
```

```{r}
NY_StopFrisk1_1
```

```{r}
NY_StopFrisk1_1$`Date of Frisk` <- str_remove(NY_StopFrisk1_1$`Date of Frisk`, "-")
NY_StopFrisk1_1$`Date of Frisk` <- str_remove(NY_StopFrisk1_1$`Date of Frisk`, "_")
NY_StopFrisk1_1$`Date of Friskd` <- str_sub(NY_StopFrisk1_1$`Date of Frisk`, 8,10 ) 
str_sub(NY_StopFrisk1_1$`Date of Friskd`, 1, 1) <- "-"
NY_StopFrisk1_1$`Date of FriskGOOD` <- str_sub(NY_StopFrisk1_1$`Date of Frisk`, 1,7 ) 
NY_StopFrisk1_1F <- NY_StopFrisk1_1 %>%
  unite(`Date of Frisk`, `Date of FriskGOOD`, `Date of Friskd`  )
```

```{r}
NY_StopFrisk1_1F
```

```{r}
NY_StopFrisk1_1F$`Date of Frisk` <- str_remove(NY_StopFrisk1_1F$`Date of Frisk`, "_")

```

```{r}
NY_StopFrisk1_1F
```


```{r}
NY_StopFrisk_2017Tidy
```



```{r}
NY_StopFrisk_2017Tidy$Age <- as.double(NY_StopFrisk_2017Tidy$Age)
NY_StopFrisk_2017Tidy$Height <- as.double(NY_StopFrisk_2017Tidy$Height)
NY_StopFrisk_2017Tidy$Weight <- as.double(NY_StopFrisk_2017Tidy$Weight)
```

```{r}
NY_StopFrisk_2017Tidy$`Date of Frisk` <- as.character(NY_StopFrisk_2017Tidy$`Date of Frisk`)
NY_StopFrisk_2017Tidy$`Date of Frisk` <- as.character(NY_StopFrisk_2017Tidy$`Date of Frisk`)

```



```{r}
NY_StopFrisk2 <- full_join(NY_StopFrisk1_1F, NY_StopFrisk_2017Tidy)

```

```{r}
NY_StopFrisk2
```



# Results

For the analysis our team utilized an alpha of 0.05.

## **In New York incidents in schools happen significantly more frequent in neighborhoods were there more African Americans**

Therefore our Hypothesis are:

H0 - There is no significant difference

HA - There is a significant difference


### Exploratory analysis

The team performed an exploratory analysis of the tidy and merged data sets in order to get familiarized with the information available in the data sets. The exploratory analysis is strongly related with the results section of the present study.


### Exploring the problem

We will create a new column in the vadir_NY_Join called "TotalIncidents"

```{r}
Vadir_NY_Join$TotalIncidents <- rowSums(Vadir_NY_Join[ ,c(10:45)])
Vadir_NY_Join$TotalIncidents
```

Now lets vizualize how is the distribution of the incidents per count:


```{r}
Vadir_NY_Join %>% group_by(County) %>% 
  ggplot(mapping = aes(x = TotalIncidents, fill =  County)) + 
  geom_histogram() +
  labs(title = "Number of Incidents per count",
       subtitle = "New York Schools",
       x = "Number of observations",
       y = "Number of Incidents",
       caption = "Source: New York City")
```


```{r}
Vadir_NY_Join %>% group_by(County) %>% 
  ggplot(mapping = aes(x = TotalIncidents, fill =  County)) + 
  geom_boxplot() +
  labs(title = "Number of Incidents per count",
       subtitle = "New York Schools",
       x = "Observations",
       y = "Counts",
       caption = "Source: New York City")
```

As we can observe the incidents are somewhat different in the different counts. Therefore, more analysis is required for further explanation of the differences.


### Exploring with numbers

```{r}
Vadir_NY_Join %>% group_by(County) %>% 
  select(TotalIncidents) %>%
  na.omit() %>%
  summarize(avgExtra = mean(TotalIncidents),
            sdExtra = sd(TotalIncidents),
            count = n())
```

The mean values and the standard deviation of the variables give a better understanding of how each county struggles with incidents such drug trafficking, robbery, and violence in general.

### Cheking for normality


Before we test the hypothesis that the number of incidents reported is different, it is necessery define if the data sets being analyzed are normally distributed.

For the statistical test we will compare the Bronx which 36% of its pupolation is African American and Richmond which 98% of its population is caucasian according to the last census. 


**Bronx**

Lets see if how is the distrbution of the incidents in the Bronx.

```{r}
TotalBronkx <- Vadir_NY_Join %>%
  select(County, TotalIncidents) %>%
  filter(County == "Bronx") %>%
  na.omit()
```


```{r}
shapiro.test(TotalBronkx$TotalIncidents)
```

The observations of the incidents in the Bronx are not normally distributed. The P-value is lower than the alpha (0.05).


**Richmond**

Lets see how is the distribution of the incidents in Richmond.

```{r}
TotalRichmond <- Vadir_NY_Join %>%
  select(County, TotalIncidents) %>%
  filter(County == "Richmond") %>%
  na.omit()
```

```{r}
shapiro.test(TotalRichmond$TotalIncidents)
```

The observations of the incidents in the Richmond are not normally distributed. The P-value is lower than the alpha (0.05).

### TEsting the hypothesis

Since both data sets were not nrmally distributed we will be utilizing a wilcox.test that will compare the two sample for significant differences.

We first create a data set for the analysis, as in the code below:
 
```{r}
HypothesisTestSchoolR <- as.tibble(TotalRichmond)
HypothesisTestSchoolB <- as.tibble(TotalBronkx)
HypothesisTestSchoolJ <- full_join(HypothesisTestSchoolR,            HypothesisTestSchoolB )                                  
  
HypothesisTestSchoolJ
```
Then we test its significance:

```{r}
HypothesisTestSchoolI <- wilcox.test(TotalIncidents ~ County, data = HypothesisTestSchoolJ,  paired = FALSE)
```
```{r}
HypothesisTestSchoolI
```

The P-value was higher than he the alpha. THerefore, we failed to reject the null Hypothesis, in other words there is no sufficient evidence to support the claim that there is a significant difference in the Total incidents.


## **In New York incidents with weapons in schools happen significantly more frequent in neighborhoods were there more African Americans**

Therefore our Hypothesis are:

H0 - There is no significant difference

HA - There is a significant difference


### Exploratory analysis

The team performed an exploratory analysis of the tidy and merged data sets in order to get familiarized with the information available in the data sets. The exploratory analysis is strongly related with the results section of the present study.


### Exploring the problem

lets vizualize how is the distribution of the incidents in which a minor was assaulted:

```{r}
Vadir_NY_Join %>% group_by(County) %>% 
  ggplot(mapping = aes(x = `Assault With Serious Physical Injury With Weapon(s)`, fill =  County)) + 
  geom_histogram() +
  labs(title = "Number of Assaults per count",
       subtitle = "Serious Physical Injury With Weapon(s)",
       x = "Number of observations",
       y = "Number of Incidents",
       caption = "Source: New York City")
```
```{r}
Vadir_NY_Join %>% group_by(County) %>% 
  ggplot(mapping = aes(x = `Assault with Physical Injury With Weapon(s)`, fill =  County)) + 
  geom_histogram() +
  labs(title = "Number of Assaults per count",
       subtitle = "Physical Injury With Weapon(s)",
       x = "Number of observations",
       y = "Number of Incidents",
       caption = "Source: New York City")
```

As we can observe the incidents are somewhat different in the different counts. Therefore, more analysis is required for further explanation of the differences.


### Exploring with numbers

We will exclude the observations that were zero.

```{r}
Vadir_NY_Join %>% select(`Assault with Physical Injury With Weapon(s)`, County) %>%
  filter(`Assault with Physical Injury With Weapon(s)` > 0) %>%
  group_by(County) %>%
  na.omit() %>%
  summarize(avgExtra = mean(`Assault with Physical Injury With Weapon(s)`),
            sdExtra = sd(`Assault with Physical Injury With Weapon(s)`),
            count = n())
```

The mean values and the standard deviation of the variables give a better understanding of how each county struggles with incidents of assaults with physical injuries. The numbers are really impressive. However we need to check its significance.


### Cheking for normality


Before we test the hypothesis that the number of incidents reported is different, it is necessery define if the data sets being analyzed are normally distributed.

For the statistical test we will compare the Bronx which 36% of its pupolation is African American and Richmond which 98% of its population is caucasian according to the last census. 


**Bronx**

Lets see how is the distrbution of the incidents in the Bronx.

```{r}
WeaponsBronkx <- Vadir_NY_Join %>%
  select(County, `Assault with Physical Injury With Weapon(s)`) %>%
  filter(County == "Bronx") %>%
  na.omit()
```


```{r}
shapiro.test(WeaponsBronkx$`Assault with Physical Injury With Weapon(s)`)
```

The observations of the incidents in the Bronx are not normally distributed. The P-value is lower than the alpha (0.05).



**Richmond**

Lets see how is the distribution of the incidents in Richmond.
```{r}
WeaponsRichmond <- Vadir_NY_Join %>%
  select(County, `Assault with Physical Injury With Weapon(s)`) %>%
  filter(County == "Richmond") %>%
  na.omit()
```


```{r}
shapiro.test(WeaponsRichmond$`Assault with Physical Injury With Weapon(s)`)
```


The observations of the incidents in the Bronx are not normally distributed. The P-value is lower than the alpha (0.05).

### TEsting the hypothesis

Since both data sets were not nrmally distributed we will be utilizing a wilcox.test that will compare the two sample for significant differences.

We first create a data set for the analysis, as in the code below:
 
```{r}
HypothesisTestSchoolWR <- as.tibble(WeaponsRichmond)
HypothesisTestSchoolWB <- as.tibble(WeaponsBronkx)
HypothesisTestSchoolWJ <- full_join(HypothesisTestSchoolWR,            HypothesisTestSchoolWB )                                  
  
HypothesisTestSchoolWJ
```


Then we test its significance:

```{r}
HypothesisTestSchoolWI <- wilcox.test(`Assault with Physical Injury With Weapon(s)` ~ County, data = HypothesisTestSchoolWJ,  paired = FALSE)
```
```{r}
HypothesisTestSchoolWI
```
Another analysis given the disparity of observations will be a probability test. 

```{r}

Vadir_NY_Join %>% select(`Assault with Physical Injury With Weapon(s)`, County) %>%
  group_by(County) %>%
  na.omit() %>%
  summarize(avgExtra = mean(`Assault with Physical Injury With Weapon(s)`),
            sdExtra = sd(`Assault with Physical Injury With Weapon(s)`),
            count = n())


```

```{r}
Vadir_NY_Join %>% select(`Assault with Physical Injury With Weapon(s)`, County) %>%
  filter(`Assault with Physical Injury With Weapon(s)` > 0) %>%
  group_by(County) %>%
  na.omit() %>%
  summarize(avgExtra = mean(`Assault with Physical Injury With Weapon(s)`),
            sdExtra = sd(`Assault with Physical Injury With Weapon(s)`),
            count = n())
```

Now we test the significance of the probabilities:

```{r}
prop.test(c(57, 331), c(207, 1109))
```

For both statistical tests the p-value was lower the alpha. Therefore, we fail to reject the null hypothesis, in other words there is note sufficient evidence to support the claim that there is a significant difference in the number of weapon incidents in the two counties.

## Time Series Analysis
#Gettin the date vecto to split into new colums Year, montnh day
```{r}
#Getting the date vector
WashDate <- data.frame(Washington_post_Shootings$date)
WashDate
```

#Spliting date variable
```{r}
#Spliting Year 
WashDate$Year <-  str_sub(WashDate$Washington_post_Shootings.date , 1, 4)
```

```{r}
#Spliting Month
WashDate$Month <-  str_sub(WashDate$Washington_post_Shootings.date , 6, 7)
```

```{r}
#Spliting Day
WashDate$Day <-  str_sub(WashDate$Washington_post_Shootings.date , 9, 10 )
```

```{r}
#Printing the splitted data
WashDate
```

#Merging data sets WashPostDates
```{r}
#Merging datased with new columns creadted for Year, Month and Day
WashPostDates <- cbind(Washington_post_Shootings,WashDate)
WashPostDates
```

#Time Series Race =White
```{r}
#Preapearing data frame for time series analysis
WashWHiteOnly <- WashPostDates %>%
  filter(race== "W")%>%
  select(Year, Month)%>%
  group_by(Year,Month)%>%
  summarise(count= n())
WashWHiteOnly 
```

```{r}
#Separating numeric vector for Time Series
QtyCasesWhite<- WashWHiteOnly$count
```


```{r}
#Getting the time seris table
TSWhite <- ts(QtyCasesWhite, start = c(2015), frequency=12)
TSWhite
```

```{r}
#Decomposition of the Time Series and visualization of the results
decomposedResWhite <- decompose(TSWhite, type="additive")
stlResWhite <- stl(TSWhite, s.window = "periodic")
plot (decomposedResWhite, type="o", col="red", lty="dashed")
```


#TimeSeries Race = Black
```{r}
#Preapearing data frame for time series analysis
WashBlackOnly <- WashPostDates %>%
  filter(race== "B")%>%
  select(Year, Month)%>%
  group_by(Year,Month)%>%
  summarise(count= n())
WashBlackOnly 
```

```{r}
#Separating numeric vector for Time Series
QtyCasesBlack <- WashBlackOnly$count
```


```{r}
#Getting the time seris table
TSBlack <- ts(QtyCasesBlack, start = c(2015), frequency=12)
TSBlack
```

```{r}
#Decomposition of the Time Series and visualization of the results
decomposedResBlack <- decompose(TSBlack, type="additive")
stlResBlack <- stl(TSBlack, s.window = "periodic")
plot (decomposedResBlack, type="o", col="red", lty="dashed")

#### The prediciton

COnstructing a model to predict the shootings involving African Americans:

```{r}
BModelShootings <- auto.arima(TSBlack)
```


```{r}
BShootingsForecast <- forecast(BModelShootings, h = 12)
BShootingsForecast
```


##### plotting the prediction

```{r}
plot(BShootingsForecast)
```

## REgression

In this study will built two models one for the schools located in the Bronx and the other for the ones located in Richmond.


**Bronx**

First lets select the data set for the analysis

```{r}
RegressionBronx <-
  Vadir_NY_Join %>% 
  filter(County == "Bronx", 
         TotalIncidents > 0,
         Enrollment < 2000) %>%
  select(Enrollment, TotalIncidents)
```

### Visualization 

lets visualize to better understand the relationship of the variables

```{r}
ggplot(data = RegressionBronx, 
       mapping = aes(
         x = Enrollment,
         y =TotalIncidents
       ))+
  geom_point() +
  geom_abline() +
  labs(title = "Relationship Enrollments VS Incidents",
  subtitle = "Bronx",
  caption = "Source: New York GoV")
```

### Checking for correlation

```{r}
cor(RegressionBronx$Enrollment, 
    RegressionBronx$TotalIncidents)
```

### non-linear 

```{r}
ggplot(data = RegressionBronx, 
       mapping = aes(
         x = Enrollment,
         y =TotalIncidents
       ))+
  geom_point() +
  geom_smooth(method = loess) +
  labs(title = "Relationship Enrollments VS Incidents",
  subtitle = "Bronx",
  caption = "Source: New York GoV")
```
### building the model

Since there was no coorrelatio between the variables, the best model will be a no-linear regression.

Our independent variable is the enrollment.

OUr dependent varible is the Incidents.

```{r}
PredicitonBronx <- loess(RegressionBronx$TotalIncidents ~ RegressionBronx$Enrollment)
```

### Testing the model

Given 1000 enrollments what would be nurbers of incidents

```{r}
testB <- 1000
```

```{r}
PredictionB <- predict(PredicitonBronx, testB)
PredictionB
```



**Richmond**

First lets select the data set for the analysis

```{r}
RegressionRichmond <-
  Vadir_NY_Join %>% 
  filter(County == "Richmond", 
         TotalIncidents > 0,
         Enrollment < 2000) %>%
  select(Enrollment, TotalIncidents)
```

### Visualization 

lets visualize to better understand the relationship of the variables

```{r}
ggplot(data = RegressionRichmond, 
       mapping = aes(
         x = Enrollment,
         y =TotalIncidents
       ))+
  geom_point() +
  geom_abline() +
  labs(title = "Relationship Enrollments VS Incidents",
  subtitle = "Richmond",
  caption = "Source: New York GoV")
```

### Checking for correlation

```{r}
cor(RegressionRichmond$Enrollment, 
    RegressionRichmond$TotalIncidents)
```

### non-linear 

```{r}
ggplot(data = RegressionRichmond, 
       mapping = aes(
         x = Enrollment,
         y =TotalIncidents
       ))+
  geom_point() +
  geom_smooth(method = loess) +
  labs(title = "Relationship Enrollments VS Incidents",
  subtitle = "Richmond",
  caption = "Source: New York GoV")
```
### building the model

Since there was no coorrelatio between the variables, the best model will be a no-linear regression.

Our independent variable is the enrollment.

OUr dependent varible is the Incidents.

```{r}
PredicitonRichmond <- loess(RegressionRichmond$TotalIncidents ~ RegressionRichmond$Enrollment)
```

### Testing the model

Given 1000 enrollments what would be nurbers of incidents

```{r}
testR <- 1000
```

```{r}
PredictionR <- predict(PredicitonRichmond, testR)
PredictionR
```


# conclusion

**The Black Lives Matter movement is a complexed issue in our society as a whole. Although, our results did not show significant difference in our Hypothesis, the problem in disparity between races is much complex than the analysis available in this study.**

**In future studies, in order to achieve meaningful results, more complex analysis must be done.**


# Refences


[1]  R Core Team (2020). R: A language and
  environment for statistical computing. R
  Foundation for Statistical Computing,
  Vienna, Austria. URL
  https://www.R-project.org/.
  
[2] RStudio Team (2020). RStudio: Integrated           Development for R. RStudio, PBC, Boston, MA URL      http://www.rstudio.com/.

[3] Software Freedom Conservancy, Inc., 2020. [Online]. Available: https://git-scm.com/

[4] GitHub, Inc., 2020. [Online]. Available: https://github.com/

[5] Wickham et al., (2019). Welcome to the
  tidyverse. Journal of Open Source
  Software, 4(43), 1686,
  https://doi.org/10.21105/joss.01686

[6] Taiyun Wei and Viliam Simko (2017). R
  package "corrplot": Visualization of a
  Correlation Matrix (Version 0.84).
  Available from
  https://github.com/taiyun/corrplot


[7] Jarek Tuszynski (2020). caTools: Tools:
  Moving Window Statistics, GIF, Base64,
  ROC AUC, etc. R package version 1.18.0.
  https://CRAN.R-project.org/package=caTools

[8] Max Kuhn (2020). caret: Classification
  and Regression Training. R package
  version 6.0-86.
  https://CRAN.R-project.org/package=caret


[9] Hyndman R, Athanasopoulos G, Bergmeir C,
  Caceres G, Chhay L, O'Hara-Wild M,
  Petropoulos F, Razbash S, Wang E, Yasmeen F
  (2020). _forecast: Forecasting functions
  for time series and linear models_. R
  package version 8.13, <URL:
  https://pkg.robjhyndman.com/forecast/>.
  
  

